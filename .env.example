# TRIZ Analysis System Environment Configuration
# Copy this file to .env and customize as needed

PROJECT_NAME="TRAICon"
DOCKER_IMAGE_BACKEND="backend"

# =============================================================================
# BACKEND API CONFIGURATION
# =============================================================================

# Use with OpenAI models
DEFAULT_MODEL="gpt-4.1-mini" # Options: gpt-5, gpt-4.1, gpt-4o-mini
DEFAULT_PROVIDER="openai"
OPENAI_API_KEY="YOUR_OPENAI_API_KEY_HERE" # Provide for LLM-based features

# Use with Anthropic models
# ANTHROPIC_API_KEY="YOUR_ANTHROPIC_API_KEY_HERE"
# DEFAULT_MODEL="claude-sonnet-4-5"
# DEFAULT_PROVIDER="anthropic"

# Use with Groq models
# GROQ_API_KEY="YOUR_GROQ_API_KEY_HERE"
# DEFAULT_MODEL="llama-3.3-70b-versatile"
# DEFAULT_PROVIDER="groq"

# Use with Ollama models
# DEFAULT_MODEL="SELECT MODEL TO USE"
# DEFAULT_PROVIDER="ollama"
# OLLAMA_BASE_URL="http://host.docker.internal:11434/v1"  # Use host.docker.internal for Docker, or http://localhost:11434/v1 for local

# Configure embedding model for semantic search
EMBEDDING_MODEL="sentence-transformers/all-MiniLM-L6-v2"
# Any model from Huggingface that does not require remote_code